{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6232032,"sourceType":"datasetVersion","datasetId":3525344}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/aliknot/facial-emotion-recognitions?scriptVersionId=233885047\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"id":"798024c4-cf9c-4a28-a37f-b240f8bca785","cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/aliknot/facial-emotion-recognitions?scriptVersionId=232682889\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"id":"be2a1849-eae3-48f8-867d-509e9c4a4c26","cell_type":"markdown","source":"# Emotion detection in Facial Recognition","metadata":{}},{"id":"9301fb81-c1ec-47a1-8d63-872e0e70cc4c","cell_type":"markdown","source":"## Clone github repo","metadata":{}},{"id":"8798a25b-0d44-455a-9d3c-7e99c3c33303","cell_type":"code","source":"!git clone https://github.com/aliknot/facial-emotion-recognition\n!git submodule update --init --recursive","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:20:32.694346Z","iopub.execute_input":"2025-04-14T15:20:32.694649Z","iopub.status.idle":"2025-04-14T15:20:49.417199Z","shell.execute_reply.started":"2025-04-14T15:20:32.694622Z","shell.execute_reply":"2025-04-14T15:20:49.416097Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'facial-emotion-recognition'...\nremote: Enumerating objects: 871, done.\u001b[K\nremote: Counting objects: 100% (352/352), done.\u001b[K\nremote: Compressing objects: 100% (342/342), done.\u001b[K\nremote: Total 871 (delta 11), reused 344 (delta 9), pack-reused 519 (from 2)\u001b[K\nReceiving objects: 100% (871/871), 599.03 MiB | 51.52 MiB/s, done.\nResolving deltas: 100% (17/17), done.\nUpdating files: 100% (162/162), done.\nfatal: not a git repository (or any parent up to mount point /kaggle)\nStopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n","output_type":"stream"}],"execution_count":1},{"id":"534e9db0-e6d6-4140-993f-e50576be5756","cell_type":"code","source":"!git clone https://github.com/DingXiaoH/RepVGG","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:20:49.418507Z","iopub.execute_input":"2025-04-14T15:20:49.418838Z","iopub.status.idle":"2025-04-14T15:20:49.934101Z","shell.execute_reply.started":"2025-04-14T15:20:49.418811Z","shell.execute_reply":"2025-04-14T15:20:49.932967Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'RepVGG'...\nremote: Enumerating objects: 581, done.\u001b[K\nremote: Counting objects: 100% (236/236), done.\u001b[K\nremote: Compressing objects: 100% (91/91), done.\u001b[K\nremote: Total 581 (delta 192), reused 180 (delta 144), pack-reused 345 (from 1)\u001b[K\nReceiving objects: 100% (581/581), 479.37 KiB | 4.75 MiB/s, done.\nResolving deltas: 100% (348/348), done.\n","output_type":"stream"}],"execution_count":2},{"id":"09db16e5-c43c-4d27-9d28-59d107612b39","cell_type":"markdown","source":"## Install essential libraries","metadata":{}},{"id":"cc1fe5b0-f4cd-4c37-9f76-8569bacfb8d8","cell_type":"code","source":"!pip install mtcnn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:20:49.936204Z","iopub.execute_input":"2025-04-14T15:20:49.936526Z","iopub.status.idle":"2025-04-14T15:20:55.567753Z","shell.execute_reply.started":"2025-04-14T15:20:49.936501Z","shell.execute_reply":"2025-04-14T15:20:55.566736Z"}},"outputs":[{"name":"stdout","text":"Collecting mtcnn\n  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from mtcnn) (1.4.2)\nCollecting lz4>=4.3.3 (from mtcnn)\n  Downloading lz4-4.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading lz4-4.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: lz4, mtcnn\nSuccessfully installed lz4-4.4.4 mtcnn-1.0.0\n","output_type":"stream"}],"execution_count":3},{"id":"a4cc1544-8759-4871-aea6-60800bf07f31","cell_type":"markdown","source":"## Import necessary libraries","metadata":{}},{"id":"e200ed62-fbc4-4f34-bfe2-246b21295a5b","cell_type":"code","source":"import os\nimport shutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nfrom mtcnn import MTCNN\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:20:55.569305Z","iopub.execute_input":"2025-04-14T15:20:55.569527Z","iopub.status.idle":"2025-04-14T15:21:18.489493Z","shell.execute_reply.started":"2025-04-14T15:20:55.569506Z","shell.execute_reply":"2025-04-14T15:21:18.488815Z"}},"outputs":[],"execution_count":4},{"id":"04c2cf7c-422f-4f72-9790-2b52b86c56cf","cell_type":"markdown","source":"## Global variables","metadata":{}},{"id":"ed731e5b-3ff0-43c1-9c4e-6af614092e94","cell_type":"code","source":"output_base_url = '/kaggle/working/'\nrepo_base_url = os.path.join(output_base_url, 'facial-emotion-recognition')\nimages_url = os.path.join(repo_base_url, 'images')\n\nrepvgg_url = os.path.join(output_base_url, 'RepVGG')\norganized_images_url = os.path.join(output_base_url, 'images_organized')\ncropped_images_url = os.path.join(output_base_url, 'cropped_images')\nresized_images_url = os.path.join(output_base_url, 'resized_images')\nnormalized_images_url = os.path.join(output_base_url, 'normalized_images')\naugmented_images_url = os.path.join(output_base_url, 'augmented_images')\nsplitted_images_url = os.path.join(output_base_url, 'splitted_images')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:21:18.490259Z","iopub.execute_input":"2025-04-14T15:21:18.490767Z","iopub.status.idle":"2025-04-14T15:21:18.495844Z","shell.execute_reply.started":"2025-04-14T15:21:18.490743Z","shell.execute_reply":"2025-04-14T15:21:18.495Z"}},"outputs":[],"execution_count":5},{"id":"661624e9-d142-49b7-ad20-770ff0a15bc0","cell_type":"code","source":"# List of emotions\nemotions = ['Happy', 'Neutral', 'Sad', 'Surprised', 'Disgust', 'Anger', 'Fear', 'Contempt']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:21:18.496941Z","iopub.execute_input":"2025-04-14T15:21:18.497281Z","iopub.status.idle":"2025-04-14T15:21:18.535181Z","shell.execute_reply.started":"2025-04-14T15:21:18.49725Z","shell.execute_reply":"2025-04-14T15:21:18.534524Z"}},"outputs":[],"execution_count":6},{"id":"c2b357fe-439e-4369-87a7-dbc614d13c72","cell_type":"markdown","source":"## Organize the photos in emotions folders","metadata":{}},{"id":"074e7be8-af80-49a9-b42e-0f9c92e1c211","cell_type":"code","source":"if not os.path.exists(organized_images_url):\n    os.makedirs(organized_images_url)\n    \n# Iterate through each numbered folder (0-18)\nfor folder_name in sorted(os.listdir(images_url)):\n    folder_path = os.path.join(images_url, folder_name)\n    if os.path.isdir(folder_path):  # Ensure it's a directory\n        \n        for img_file in os.listdir(folder_path):\n            img_path = os.path.join(folder_path, img_file)\n            \n            if os.path.isfile(img_path):  # Ensure it's a file\n                emotion_label = img_file.split('.')[0]  # Extract the emotion label\n                \n                if emotion_label in emotions:\n                    destination_folder = os.path.join(organized_images_url, emotion_label)\n                    if not os.path.exists(destination_folder):\n                        os.makedirs(destination_folder)\n                    \n                    # Rename the image with folder name to avoid duplicates\n                    new_img_name = f\"{folder_name}_{img_file}\"\n                    destination_path = os.path.join(destination_folder, new_img_name)\n                    \n                    shutil.copy(img_path, destination_path)  # Copy instead of move\n                    print(f\"Copied {img_file} to {destination_path}\")\n\nprint(\"Dataset successfully organized!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:21:18.535947Z","iopub.execute_input":"2025-04-14T15:21:18.536225Z","iopub.status.idle":"2025-04-14T15:21:18.930563Z","shell.execute_reply.started":"2025-04-14T15:21:18.536204Z","shell.execute_reply":"2025-04-14T15:21:18.929803Z"}},"outputs":[{"name":"stdout","text":"Copied Fear.jpg to /kaggle/working/images_organized/Fear/0_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/0_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/0_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/0_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/0_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/0_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/0_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/0_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/1_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/1_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/1_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/1_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/1_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/1_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/1_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/1_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/10_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/10_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/10_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/10_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/10_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/10_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/10_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/10_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/11_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/11_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/11_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/11_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/11_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/11_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/11_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/11_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/12_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/12_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/12_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/12_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/12_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/12_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/12_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/12_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/13_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/13_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/13_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/13_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/13_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/13_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/13_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/13_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/14_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/14_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/14_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/14_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/14_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/14_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/14_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/14_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/15_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/15_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/15_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/15_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/15_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/15_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/15_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/15_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/16_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/16_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/16_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/16_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/16_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/16_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/16_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/16_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/17_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/17_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/17_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/17_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/17_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/17_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/17_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/17_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/18_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/18_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/18_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/18_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/18_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/18_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/18_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/18_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/2_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/2_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/2_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/2_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/2_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/2_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/2_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/2_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/3_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/3_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/3_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/3_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/3_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/3_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/3_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/3_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/4_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/4_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/4_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/4_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/4_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/4_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/4_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/4_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/5_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/5_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/5_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/5_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/5_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/5_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/5_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/5_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/6_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/6_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/6_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/6_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/6_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/6_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/6_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/6_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/7_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/7_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/7_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/7_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/7_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/7_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/7_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/7_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/8_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/8_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/8_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/8_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/8_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/8_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/8_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/8_Contempt.jpg\nCopied Fear.jpg to /kaggle/working/images_organized/Fear/9_Fear.jpg\nCopied Surprised.jpg to /kaggle/working/images_organized/Surprised/9_Surprised.jpg\nCopied Neutral.jpg to /kaggle/working/images_organized/Neutral/9_Neutral.jpg\nCopied Happy.jpg to /kaggle/working/images_organized/Happy/9_Happy.jpg\nCopied Anger.jpg to /kaggle/working/images_organized/Anger/9_Anger.jpg\nCopied Disgust.jpg to /kaggle/working/images_organized/Disgust/9_Disgust.jpg\nCopied Sad.jpg to /kaggle/working/images_organized/Sad/9_Sad.jpg\nCopied Contempt.jpg to /kaggle/working/images_organized/Contempt/9_Contempt.jpg\nDataset successfully organized!\n","output_type":"stream"}],"execution_count":7},{"id":"74e495b7-ca19-48a7-bb05-6c39db795586","cell_type":"markdown","source":"## Put the organized dataset in df","metadata":{}},{"id":"b780d4db-82b8-42fb-b8a2-2b0564a17539","cell_type":"code","source":"# Initialize an empty list to store data\ndata = []\n\n# Iterate over emotion folders\nfor emotion in emotions:\n    folder_path = os.path.join(organized_images_url, emotion)\n    if os.path.exists(folder_path):\n        for file_name in os.listdir(folder_path):\n            if file_name.lower().endswith('.jpg'):\n                image_path = os.path.join(folder_path, file_name)\n                folder_name = file_name.split('_')[0]  # Extracting folder number from filename\n                \n                data.append({\n                    'image_path': image_path,\n                    'emotion': emotion,\n                    'folder': folder_name\n                })\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Display first few rows\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:21:18.932644Z","iopub.execute_input":"2025-04-14T15:21:18.932858Z","iopub.status.idle":"2025-04-14T15:21:18.960982Z","shell.execute_reply.started":"2025-04-14T15:21:18.93284Z","shell.execute_reply":"2025-04-14T15:21:18.960315Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                          image_path emotion folder\n0  /kaggle/working/images_organized/Happy/6_Happy...   Happy      6\n1  /kaggle/working/images_organized/Happy/0_Happy...   Happy      0\n2  /kaggle/working/images_organized/Happy/9_Happy...   Happy      9\n3  /kaggle/working/images_organized/Happy/16_Happ...   Happy     16\n4  /kaggle/working/images_organized/Happy/17_Happ...   Happy     17","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_path</th>\n      <th>emotion</th>\n      <th>folder</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/working/images_organized/Happy/6_Happy...</td>\n      <td>Happy</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/working/images_organized/Happy/0_Happy...</td>\n      <td>Happy</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/working/images_organized/Happy/9_Happy...</td>\n      <td>Happy</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/working/images_organized/Happy/16_Happ...</td>\n      <td>Happy</td>\n      <td>16</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/working/images_organized/Happy/17_Happ...</td>\n      <td>Happy</td>\n      <td>17</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"id":"94722ce9-2afa-4253-bfa4-2a717f5a5e2b","cell_type":"markdown","source":"## Visualize all of the imgaes per emotion","metadata":{}},{"id":"5f5ea1d7-0d4d-4da0-ae28-12ca085763b7","cell_type":"code","source":"\"\"\"\n# Count number of images per emotion\nemotion_counts = df['emotion'].value_counts()\n\n# Plot the data\nplt.figure(figsize=(10, 5))\nsns.barplot(x=emotion_counts.index, y=emotion_counts.values, palette=\"viridis\", hue=emotion_counts.index, legend=False)\n\n# Formatting\nplt.title(\"Number of Images per Emotion\", fontsize=14)\nplt.xlabel(\"Emotion\", fontsize=12)\nplt.ylabel(\"Number of Images\", fontsize=12)\nplt.xticks(rotation=45)\nplt.grid(axis='y', linestyle='--', alpha=0.7)\n\n# Show the plot\nplt.show()\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:21:18.962029Z","iopub.execute_input":"2025-04-14T15:21:18.962333Z","iopub.status.idle":"2025-04-14T15:21:18.967044Z","shell.execute_reply.started":"2025-04-14T15:21:18.962312Z","shell.execute_reply":"2025-04-14T15:21:18.966165Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'\\n# Count number of images per emotion\\nemotion_counts = df[\\'emotion\\'].value_counts()\\n\\n# Plot the data\\nplt.figure(figsize=(10, 5))\\nsns.barplot(x=emotion_counts.index, y=emotion_counts.values, palette=\"viridis\", hue=emotion_counts.index, legend=False)\\n\\n# Formatting\\nplt.title(\"Number of Images per Emotion\", fontsize=14)\\nplt.xlabel(\"Emotion\", fontsize=12)\\nplt.ylabel(\"Number of Images\", fontsize=12)\\nplt.xticks(rotation=45)\\nplt.grid(axis=\\'y\\', linestyle=\\'--\\', alpha=0.7)\\n\\n# Show the plot\\nplt.show()\\n'"},"metadata":{}}],"execution_count":9},{"id":"b27dc501-03e6-4ad0-aa8f-81c7f8afe27a","cell_type":"markdown","source":"## Crop all of the images, focusing on the face","metadata":{}},{"id":"0a59ed67-8598-49c3-9deb-cbb288394d9e","cell_type":"code","source":"# Initialize MTCNN detector\ndetector = MTCNN(device = 'GPU')\n\n# Check if the the images are already cropped or not\nif not os.path.exists(cropped_images_url):\n    os.makedirs(cropped_images_url)\n    \n    # Process each emotion folder\n    for emotion in emotions:\n        emotion_folder = os.path.join(organized_images_url, emotion)\n        cropped_emotion_folder = os.path.join(cropped_images_url, emotion)\n        \n        os.makedirs(cropped_emotion_folder, exist_ok=True)\n    \n        for img_name in os.listdir(emotion_folder):\n            img_path = os.path.join(emotion_folder, img_name)\n            image = cv2.imread(img_path)\n    \n            if image is None:\n                print(f\"Skipping {img_name}, not a valid image.\")\n                continue\n    \n            # Convert image to RGB for MTCNN\n            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    \n            # Detect faces\n            detections = detector.detect_faces(image_rgb)\n    \n            if len(detections) == 0:\n                print(f\"No face detected in {img_name}, skipping...\")\n                continue\n    \n            # Get the bounding box of the first detected face\n            x, y, w, h = detections[0]['box']\n            cropped_face = image[y:y+h, x:x+w]\n    \n            # Save cropped image\n            cropped_img_path = os.path.join(cropped_emotion_folder, img_name)\n            cv2.imwrite(cropped_img_path, cropped_face)\n            print(\"Cropped the image\", img_name, emotion)\n\n    print(\"Face cropping completed!\")\nelse:\n    print(\"Images are already cropped!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:21:18.967836Z","iopub.execute_input":"2025-04-14T15:21:18.968173Z","iopub.status.idle":"2025-04-14T15:46:45.605101Z","shell.execute_reply.started":"2025-04-14T15:21:18.968138Z","shell.execute_reply":"2025-04-14T15:46:45.604181Z"}},"outputs":[{"name":"stdout","text":"Cropped the image 6_Happy.jpg Happy\nCropped the image 0_Happy.jpg Happy\nCropped the image 9_Happy.jpg Happy\nCropped the image 16_Happy.jpg Happy\nCropped the image 17_Happy.jpg Happy\nCropped the image 4_Happy.jpg Happy\nCropped the image 14_Happy.jpg Happy\nCropped the image 3_Happy.jpg Happy\nCropped the image 11_Happy.jpg Happy\nCropped the image 10_Happy.jpg Happy\nCropped the image 15_Happy.jpg Happy\nCropped the image 12_Happy.jpg Happy\nCropped the image 7_Happy.jpg Happy\nCropped the image 5_Happy.jpg Happy\nCropped the image 1_Happy.jpg Happy\nCropped the image 8_Happy.jpg Happy\nCropped the image 13_Happy.jpg Happy\nCropped the image 18_Happy.jpg Happy\nCropped the image 2_Happy.jpg Happy\nCropped the image 3_Neutral.jpg Neutral\nCropped the image 2_Neutral.jpg Neutral\nCropped the image 12_Neutral.jpg Neutral\nCropped the image 10_Neutral.jpg Neutral\nCropped the image 18_Neutral.jpg Neutral\nCropped the image 5_Neutral.jpg Neutral\nCropped the image 7_Neutral.jpg Neutral\nCropped the image 13_Neutral.jpg Neutral\nCropped the image 11_Neutral.jpg Neutral\nCropped the image 4_Neutral.jpg Neutral\nCropped the image 1_Neutral.jpg Neutral\nCropped the image 16_Neutral.jpg Neutral\nCropped the image 17_Neutral.jpg Neutral\nCropped the image 14_Neutral.jpg Neutral\nCropped the image 9_Neutral.jpg Neutral\nCropped the image 0_Neutral.jpg Neutral\nCropped the image 15_Neutral.jpg Neutral\nCropped the image 6_Neutral.jpg Neutral\nCropped the image 8_Neutral.jpg Neutral\nCropped the image 3_Sad.jpg Sad\nCropped the image 14_Sad.jpg Sad\nCropped the image 9_Sad.jpg Sad\nCropped the image 0_Sad.jpg Sad\nCropped the image 1_Sad.jpg Sad\nCropped the image 11_Sad.jpg Sad\nCropped the image 15_Sad.jpg Sad\nCropped the image 2_Sad.jpg Sad\nCropped the image 6_Sad.jpg Sad\nCropped the image 5_Sad.jpg Sad\nCropped the image 12_Sad.jpg Sad\nCropped the image 18_Sad.jpg Sad\nCropped the image 10_Sad.jpg Sad\nCropped the image 8_Sad.jpg Sad\nCropped the image 4_Sad.jpg Sad\nCropped the image 7_Sad.jpg Sad\nCropped the image 17_Sad.jpg Sad\nCropped the image 16_Sad.jpg Sad\nCropped the image 13_Sad.jpg Sad\nCropped the image 12_Surprised.jpg Surprised\nCropped the image 9_Surprised.jpg Surprised\nCropped the image 10_Surprised.jpg Surprised\nCropped the image 4_Surprised.jpg Surprised\nCropped the image 17_Surprised.jpg Surprised\nCropped the image 1_Surprised.jpg Surprised\nCropped the image 0_Surprised.jpg Surprised\nCropped the image 16_Surprised.jpg Surprised\nCropped the image 14_Surprised.jpg Surprised\nCropped the image 2_Surprised.jpg Surprised\nCropped the image 18_Surprised.jpg Surprised\nCropped the image 11_Surprised.jpg Surprised\nCropped the image 13_Surprised.jpg Surprised\nCropped the image 3_Surprised.jpg Surprised\nCropped the image 6_Surprised.jpg Surprised\nCropped the image 7_Surprised.jpg Surprised\nCropped the image 15_Surprised.jpg Surprised\nCropped the image 5_Surprised.jpg Surprised\nCropped the image 8_Surprised.jpg Surprised\nCropped the image 11_Disgust.jpg Disgust\nCropped the image 13_Disgust.jpg Disgust\nCropped the image 4_Disgust.jpg Disgust\nCropped the image 7_Disgust.jpg Disgust\nCropped the image 10_Disgust.jpg Disgust\nCropped the image 18_Disgust.jpg Disgust\nCropped the image 3_Disgust.jpg Disgust\nCropped the image 17_Disgust.jpg Disgust\nCropped the image 12_Disgust.jpg Disgust\nCropped the image 1_Disgust.jpg Disgust\nCropped the image 5_Disgust.jpg Disgust\nCropped the image 9_Disgust.jpg Disgust\nCropped the image 14_Disgust.jpg Disgust\nCropped the image 0_Disgust.jpg Disgust\nCropped the image 2_Disgust.jpg Disgust\nCropped the image 16_Disgust.jpg Disgust\nCropped the image 15_Disgust.jpg Disgust\nCropped the image 6_Disgust.jpg Disgust\nCropped the image 8_Disgust.jpg Disgust\nCropped the image 8_Anger.jpg Anger\nCropped the image 16_Anger.jpg Anger\nCropped the image 3_Anger.jpg Anger\nCropped the image 17_Anger.jpg Anger\nCropped the image 4_Anger.jpg Anger\nCropped the image 18_Anger.jpg Anger\nCropped the image 0_Anger.jpg Anger\nCropped the image 11_Anger.jpg Anger\nCropped the image 1_Anger.jpg Anger\nCropped the image 9_Anger.jpg Anger\nCropped the image 10_Anger.jpg Anger\nCropped the image 6_Anger.jpg Anger\nCropped the image 15_Anger.jpg Anger\nCropped the image 5_Anger.jpg Anger\nCropped the image 2_Anger.jpg Anger\nCropped the image 13_Anger.jpg Anger\nCropped the image 14_Anger.jpg Anger\nCropped the image 12_Anger.jpg Anger\nCropped the image 7_Anger.jpg Anger\nCropped the image 4_Fear.jpg Fear\nCropped the image 13_Fear.jpg Fear\nCropped the image 10_Fear.jpg Fear\nCropped the image 12_Fear.jpg Fear\nCropped the image 17_Fear.jpg Fear\nCropped the image 3_Fear.jpg Fear\nCropped the image 9_Fear.jpg Fear\nCropped the image 6_Fear.jpg Fear\nCropped the image 8_Fear.jpg Fear\nCropped the image 0_Fear.jpg Fear\nCropped the image 5_Fear.jpg Fear\nCropped the image 7_Fear.jpg Fear\nCropped the image 18_Fear.jpg Fear\nCropped the image 2_Fear.jpg Fear\nCropped the image 11_Fear.jpg Fear\nCropped the image 15_Fear.jpg Fear\nCropped the image 1_Fear.jpg Fear\nCropped the image 16_Fear.jpg Fear\nCropped the image 14_Fear.jpg Fear\nCropped the image 3_Contempt.jpg Contempt\nCropped the image 14_Contempt.jpg Contempt\nCropped the image 12_Contempt.jpg Contempt\nCropped the image 2_Contempt.jpg Contempt\nCropped the image 8_Contempt.jpg Contempt\nCropped the image 1_Contempt.jpg Contempt\nCropped the image 10_Contempt.jpg Contempt\nCropped the image 16_Contempt.jpg Contempt\nCropped the image 0_Contempt.jpg Contempt\nCropped the image 6_Contempt.jpg Contempt\nCropped the image 4_Contempt.jpg Contempt\nCropped the image 11_Contempt.jpg Contempt\nCropped the image 18_Contempt.jpg Contempt\nCropped the image 17_Contempt.jpg Contempt\nCropped the image 9_Contempt.jpg Contempt\nCropped the image 15_Contempt.jpg Contempt\nCropped the image 7_Contempt.jpg Contempt\nCropped the image 5_Contempt.jpg Contempt\nCropped the image 13_Contempt.jpg Contempt\nFace cropping completed!\n","output_type":"stream"}],"execution_count":10},{"id":"26aad4f8-0062-46fc-8515-d204a3d94641","cell_type":"markdown","source":"## Resize Images","metadata":{}},{"id":"b76d232b-baea-4a8c-b36b-65e03a1a0289","cell_type":"code","source":"# Target size for RepVGG\ntarget_size = (224, 224)\n\n# Check if the the images are already resized or not\nif not os.path.exists(resized_images_url):\n    os.makedirs(resized_images_url)\n    \n    # Process each emotion folder\n    for emotion in emotions:\n        emotion_folder = os.path.join(cropped_images_url, emotion)\n        resized_emotion_folder = os.path.join(resized_images_url, emotion)\n        \n        os.makedirs(resized_emotion_folder, exist_ok=True)\n    \n        for img_name in os.listdir(emotion_folder):\n            img_path = os.path.join(emotion_folder, img_name)\n            image = cv2.imread(img_path)\n    \n            if image is None:\n                print(f\"Skipping {img_name}, not a valid image.\")\n                continue\n    \n            # Resize the image\n            resized_image = cv2.resize(image, target_size)\n    \n            # Save the resized image\n            resized_img_path = os.path.join(resized_emotion_folder, img_name)\n            cv2.imwrite(resized_img_path, resized_image)\n\n    print(\"Image resizing completed! All images are now 224x224.\")\nelse:\n    print(\"Images are already resized!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:46:45.606159Z","iopub.execute_input":"2025-04-14T15:46:45.606492Z","iopub.status.idle":"2025-04-14T15:46:47.379957Z","shell.execute_reply.started":"2025-04-14T15:46:45.606458Z","shell.execute_reply":"2025-04-14T15:46:47.379179Z"}},"outputs":[{"name":"stdout","text":"Image resizing completed! All images are now 224x224.\n","output_type":"stream"}],"execution_count":11},{"id":"34c585de-9ed9-4f3f-8a83-52283f701cda","cell_type":"markdown","source":"## Normalize Images","metadata":{}},{"id":"37fffb4b-ee27-4f89-9896-2eba70bfdbc4","cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, image_dir, transform=None):\n        self.image_dir = image_dir\n        self.transform = transform\n        self.image_paths = []\n        for root, _, files in os.walk(image_dir):\n            for file in files:\n                if file.endswith(('png', 'jpg', 'jpeg')):\n                    self.image_paths.append(os.path.join(root, file))\n\n    def __len__(self):\n        return len(self.image_paths)\n\n    def __getitem__(self, idx):\n        img_path = self.image_paths[idx]\n        image = Image.open(img_path).convert('RGB')\n        if self.transform:\n            image = self.transform(image)\n        return image\n\n# Define a transform to convert images to tensors without normalization\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\n# Create the dataset and dataloader\ndataset = ImageDataset(image_dir=resized_images_url, transform=transform)\ndataloader = DataLoader(dataset, batch_size=64, shuffle=False)\n\n# Function to calculate mean and std\ndef calculate_mean_std(loader):\n    mean = 0.0\n    std = 0.0\n    total_samples = 0\n    for images in loader:\n        batch_samples = images.size(0)\n        images = images.view(batch_samples, images.size(1), -1)\n        mean += images.mean(2).sum(0)\n        std += images.std(2).sum(0)\n        total_samples += batch_samples\n\n    mean /= total_samples\n    std /= total_samples\n    return mean, std\n\n# Calculate mean and std\nmean, std = calculate_mean_std(dataloader)\nprint(f\"Calculated Mean: {mean}\")\nprint(f\"Calculated Std: {std}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:46:47.380974Z","iopub.execute_input":"2025-04-14T15:46:47.381325Z","iopub.status.idle":"2025-04-14T15:46:47.99197Z","shell.execute_reply.started":"2025-04-14T15:46:47.381291Z","shell.execute_reply":"2025-04-14T15:46:47.991008Z"}},"outputs":[{"name":"stdout","text":"Calculated Mean: tensor([0.6104, 0.4769, 0.4438])\nCalculated Std: tensor([0.1794, 0.1767, 0.1775])\n","output_type":"stream"}],"execution_count":12},{"id":"7fe8fda0-ed33-4726-b503-20ad8943a5e6","cell_type":"code","source":"# Define the normalization transform\nnormalize = transforms.Normalize(mean=mean, std=std)\n\n# Define the complete transform pipeline\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    normalize,\n])\n\n# Remove the directory if it exists to avoid duplicates\nif os.path.exists(normalized_images_url):\n    shutil.rmtree(normalized_images_url)\nos.makedirs(normalized_images_url)\n\n# Normalize and save images\nfor img_rel_path in dataset.image_paths:\n    img = Image.open(img_rel_path).convert('RGB')\n    img_tensor = transform(img)\n    # Convert tensor back to PIL Image\n    img_normalized = transforms.ToPILImage()(img_tensor)\n    # Define the path to save the normalized image\n    relative_path = os.path.relpath(img_rel_path, resized_images_url)\n    save_path = os.path.join(normalized_images_url, relative_path)\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    img_normalized.save(save_path)\n\nprint(\"All images have been normalized and saved.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T15:46:47.992831Z","iopub.execute_input":"2025-04-14T15:46:47.993164Z","iopub.status.idle":"2025-04-14T15:46:48.562119Z","shell.execute_reply.started":"2025-04-14T15:46:47.99314Z","shell.execute_reply":"2025-04-14T15:46:48.561156Z"}},"outputs":[{"name":"stdout","text":"All images have been normalized and saved.\n","output_type":"stream"}],"execution_count":13},{"id":"59a6bc33-b645-4836-abc3-7873514874f8","cell_type":"markdown","source":"## Data Augmentation","metadata":{}},{"id":"ce86c9e1-3f01-4f43-bca4-51be28612ca5","cell_type":"code","source":"import os\nfrom PIL import Image\nfrom torchvision import transforms\nimport torch\n\ndata_augmentation = transforms.Compose([\n    transforms.RandomResizedCrop(size=(224, 224), scale=(0.9, 1.0), ratio=(0.9, 1.1)),\n    transforms.RandomRotation(degrees=30),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n    transforms.ToTensor(),\n    transforms.Lambda(lambda x: x + 0.05 * torch.randn_like(x)),\n    transforms.RandomPerspective(distortion_scale=0.2, p=0.5)\n])\n\n# Number of augmented images to generate per original image\nnum_augmented_images = 20\n\n# Iterate over each emotion category\nfor emotion in os.listdir(normalized_images_url):\n    emotion_dir = os.path.join(normalized_images_url, emotion)\n    if not os.path.isdir(emotion_dir):\n        continue\n\n    # Create corresponding directory in the output folder\n    output_emotion_dir = os.path.join(augmented_images_url, emotion)\n    os.makedirs(output_emotion_dir, exist_ok=True)\n\n    # Process each image in the current emotion directory\n    for img_name in os.listdir(emotion_dir):\n        img_path = os.path.join(emotion_dir, img_name)\n        if not os.path.isfile(img_path):\n            continue\n\n        # Load the image\n        image = Image.open(img_path).convert('RGB')\n\n        # Generate augmented images\n        for i in range(num_augmented_images):\n            augmented_image = data_augmentation(image)\n            augmented_image_pil = transforms.ToPILImage()(augmented_image)\n\n            # Save the augmented image\n            base_name, ext = os.path.splitext(img_name)\n            augmented_img_name = f\"{base_name}_aug_{i}{ext}\"\n            augmented_img_path = os.path.join(output_emotion_dir, augmented_img_name)\n            augmented_image_pil.save(augmented_img_path)\n\nprint(\"Data augmentation complete. Augmented images are saved in:\", augmented_images_url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:44:29.977082Z","iopub.execute_input":"2025-04-14T16:44:29.977398Z","iopub.status.idle":"2025-04-14T16:45:05.979905Z","shell.execute_reply.started":"2025-04-14T16:44:29.977367Z","shell.execute_reply":"2025-04-14T16:45:05.978875Z"}},"outputs":[{"name":"stdout","text":"Data augmentation complete. Augmented images are saved in: /kaggle/working/augmented_images\n","output_type":"stream"}],"execution_count":33},{"id":"f7083376-1770-4130-b7ff-0417e7c97c1c","cell_type":"markdown","source":"## Split the Dataset","metadata":{}},{"id":"a46dded8-b658-4255-97ef-302d451e359b","cell_type":"code","source":"import os\nimport shutil\nimport random\n\n# Set the seed for reproducibility\nrandom.seed(1337)\n\n# Define paths\ntrain_dir = os.path.join(splitted_images_url, 'train')\nval_dir = os.path.join(splitted_images_url, 'val')\ntest_dir = os.path.join(splitted_images_url, 'test')\n\n# Define split ratios\ntrain_ratio = 0.7\nval_ratio = 0.15\ntest_ratio = 0.15\n\n# Create directories for splits\nfor dir in [train_dir, val_dir, test_dir]:\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n\n# Iterate over each class folder\nfor class_name in os.listdir(augmented_images_url):\n    class_dir = os.path.join(augmented_images_url, class_name)\n    if os.path.isdir(class_dir):\n        images = os.listdir(class_dir)\n        random.shuffle(images)\n        total_images = len(images)\n        train_end = int(train_ratio * total_images)\n        val_end = train_end + int(val_ratio * total_images)\n\n        # Define paths for each split\n        train_class_dir = os.path.join(train_dir, class_name)\n        val_class_dir = os.path.join(val_dir, class_name)\n        test_class_dir = os.path.join(test_dir, class_name)\n\n        # Create class directories if they don't exist\n        for dir in [train_class_dir, val_class_dir, test_class_dir]:\n            if not os.path.exists(dir):\n                os.makedirs(dir)\n\n        # Copy images to respective directories\n        for i, image in enumerate(images):\n            src = os.path.join(class_dir, image)\n            if i < train_end:\n                dst = os.path.join(train_class_dir, image)\n            elif i < val_end:\n                dst = os.path.join(val_class_dir, image)\n            else:\n                dst = os.path.join(test_class_dir, image)\n            shutil.copyfile(src, dst)\n\nprint(\"Dataset successfully split into train, validation, and test sets.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:45:12.870839Z","iopub.execute_input":"2025-04-14T16:45:12.871196Z","iopub.status.idle":"2025-04-14T16:45:13.253197Z","shell.execute_reply.started":"2025-04-14T16:45:12.871167Z","shell.execute_reply":"2025-04-14T16:45:13.252369Z"}},"outputs":[{"name":"stdout","text":"Dataset successfully split into train, validation, and test sets.\n","output_type":"stream"}],"execution_count":34},{"id":"55625dc8-767d-4e76-b7c5-bbaeb2db44c0","cell_type":"markdown","source":"## Model","metadata":{}},{"id":"e610bd32-935d-46c0-b48f-e7e4e7ae400c","cell_type":"code","source":"import os\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Define paths\ntrain_dir = os.path.join(splitted_images_url, 'train')\nval_dir = os.path.join(splitted_images_url, 'val')\ntest_dir = os.path.join(splitted_images_url, 'test')\n\n# Define transform\ntransform = transforms.Compose([\n    transforms.ToTensor()\n    # No normalization, because your images are already normalized\n])\n\n# Load datasets\ntrain_dataset = datasets.ImageFolder(train_dir, transform=transform)\nval_dataset = datasets.ImageFolder(val_dir, transform=transform)\ntest_dataset = datasets.ImageFolder(test_dir, transform=transform)\n\n# DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:48:18.867437Z","iopub.execute_input":"2025-04-14T16:48:18.867788Z","iopub.status.idle":"2025-04-14T16:48:18.886169Z","shell.execute_reply.started":"2025-04-14T16:48:18.867766Z","shell.execute_reply":"2025-04-14T16:48:18.885363Z"}},"outputs":[],"execution_count":35},{"id":"dec3a1a3-d1f9-4531-a7be-8f47836f676a","cell_type":"code","source":"import importlib.util\nimport sys\n\n# Load se_block.py first\nse_block_path = '/kaggle/working/RepVGG/se_block.py'\nse_spec = importlib.util.spec_from_file_location(\"se_block\", se_block_path)\nse_block = importlib.util.module_from_spec(se_spec)\nsys.modules[\"se_block\"] = se_block\nse_spec.loader.exec_module(se_block)\n\n# Now load repvgg.py (it depends on se_block being loaded first)\nrepvgg_path = '/kaggle/working/RepVGG/repvgg.py'\nrepvgg_spec = importlib.util.spec_from_file_location(\"repvgg\", repvgg_path)\nrepvgg = importlib.util.module_from_spec(repvgg_spec)\nsys.modules[\"repvgg\"] = repvgg\nrepvgg_spec.loader.exec_module(repvgg)\n\n# Example use:\nmodel = repvgg.create_RepVGG_A0()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:48:38.997099Z","iopub.execute_input":"2025-04-14T16:48:38.997401Z","iopub.status.idle":"2025-04-14T16:48:39.092326Z","shell.execute_reply.started":"2025-04-14T16:48:38.99738Z","shell.execute_reply":"2025-04-14T16:48:39.091493Z"}},"outputs":[{"name":"stdout","text":"RepVGG Block, identity =  None\nRepVGG Block, identity =  None\nRepVGG Block, identity =  BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  None\nRepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  None\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  None\n","output_type":"stream"}],"execution_count":36},{"id":"68632197-b9c4-446d-8478-9f52c9db2f4f","cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n# Use the dynamically imported module from earlier\nmodel = repvgg.create_RepVGG_A0(deploy=False)\n\n# Load pretrained weights (assuming this file is already in your working directory)\ncheckpoint = torch.load('/kaggle/working/facial-emotion-recognition/RepVGG-A0-train.pth')\n\n# You may need to set strict=False if fine-tuning:\nmodel.load_state_dict(checkpoint, strict=False)\n\n# Modify the classifier to match the number of classes\nnum_classes = len(train_dataset.classes)\nmodel.linear = nn.Linear(model.linear.in_features, num_classes)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:48:42.157922Z","iopub.execute_input":"2025-04-14T16:48:42.158269Z","iopub.status.idle":"2025-04-14T16:48:42.293656Z","shell.execute_reply.started":"2025-04-14T16:48:42.158242Z","shell.execute_reply":"2025-04-14T16:48:42.292737Z"}},"outputs":[{"name":"stdout","text":"RepVGG Block, identity =  None\nRepVGG Block, identity =  None\nRepVGG Block, identity =  BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  None\nRepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  None\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\nRepVGG Block, identity =  None\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-37-63d03b46e68f>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load('/kaggle/working/facial-emotion-recognition/RepVGG-A0-train.pth')\n","output_type":"stream"}],"execution_count":37},{"id":"ab7a6538-dbd5-4397-a31a-cff85ac5257d","cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:48:45.985481Z","iopub.execute_input":"2025-04-14T16:48:45.985759Z","iopub.status.idle":"2025-04-14T16:48:45.992881Z","shell.execute_reply.started":"2025-04-14T16:48:45.985738Z","shell.execute_reply":"2025-04-14T16:48:45.991971Z"}},"outputs":[],"execution_count":38},{"id":"e523be4e-1295-453a-b09e-107b9fd8a1b1","cell_type":"code","source":"num_epochs = 10\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    running_loss = 0.0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n\n    print(f\"Epoch {epoch+1}, Training Loss: {running_loss / len(train_loader)}\")\n\n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    print(f\"Validation Loss: {val_loss / len(val_loader)}, Accuracy: {100 * correct / total:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:48:49.614963Z","iopub.execute_input":"2025-04-14T16:48:49.61528Z","iopub.status.idle":"2025-04-14T16:51:04.055384Z","shell.execute_reply.started":"2025-04-14T16:48:49.615258Z","shell.execute_reply":"2025-04-14T16:51:04.054516Z"}},"outputs":[{"name":"stdout","text":"Epoch 1, Training Loss: 1.8894508659065543\nValidation Loss: 3.197407750856309, Accuracy: 20.24%\nEpoch 2, Training Loss: 1.2244871413552916\nValidation Loss: 2.0404158319745744, Accuracy: 41.70%\nEpoch 3, Training Loss: 0.769019621920276\nValidation Loss: 1.5716947210686547, Accuracy: 49.92%\nEpoch 4, Training Loss: 0.41880448220612165\nValidation Loss: 2.0991351377396357, Accuracy: 45.66%\nEpoch 5, Training Loss: 0.2994938709519126\nValidation Loss: 1.2849552759102412, Accuracy: 62.25%\nEpoch 6, Training Loss: 0.2023234710871399\nValidation Loss: 2.1298841028696014, Accuracy: 56.01%\nEpoch 7, Training Loss: 0.19319157941000803\nValidation Loss: 1.342827163991474, Accuracy: 59.51%\nEpoch 8, Training Loss: 0.17814948511394588\nValidation Loss: 2.313666349365598, Accuracy: 51.60%\nEpoch 9, Training Loss: 0.1692631199665658\nValidation Loss: 1.391498293195452, Accuracy: 63.47%\nEpoch 10, Training Loss: 0.12736965233290737\nValidation Loss: 2.0830433056468056, Accuracy: 57.23%\n","output_type":"stream"}],"execution_count":39},{"id":"ab567a15-d6d8-41fd-8fd1-3a2b77eebc20","cell_type":"code","source":"model.eval()\ntest_loss = 0.0\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for inputs, labels in test_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        test_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\nprint(f\"Test Loss: {test_loss/len(test_loader)}, Accuracy: {100 * correct / total}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-14T16:51:14.587217Z","iopub.execute_input":"2025-04-14T16:51:14.587535Z","iopub.status.idle":"2025-04-14T16:51:16.103988Z","shell.execute_reply.started":"2025-04-14T16:51:14.58751Z","shell.execute_reply":"2025-04-14T16:51:16.103262Z"}},"outputs":[{"name":"stdout","text":"Test Loss: 1.885209125422296, Accuracy: 58.850931677018636%\n","output_type":"stream"}],"execution_count":40},{"id":"b75593ab-bee9-40ea-9beb-8d3cc99902ae","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}